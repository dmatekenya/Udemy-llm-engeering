{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e36b141",
   "metadata": {},
   "source": [
    "## Comparing Chichewa Speech-to-Text: Open-Source Whisper vs OpenAI API\n",
    "\n",
    "This notebook presents a direct comparison of speech-to-text performance for Chichewa (Chicheŵa) using the open-source Whisper model and OpenAI’s hosted transcription models. The focus is strictly on evaluating transcription accuracy and qualitative differences between the two approaches when applied to the same audio samples. No fine-tuning or model adaptation is performed; instead, both systems are assessed in their default configurations to provide a clear, practical baseline comparison for Chichewa speech recognition in a low-resource language setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8407ee5",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af620dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, SVG\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, WhisperProcessor, WhisperForConditionalGeneration, TextStreamer, BitsAndBytesConfig, pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65638eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "MPS built: True\n",
      "MPS available: True\n",
      "MPS test successful ✅\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# TEST FOR MPS (MAC GPU) SUPPORT\n",
    "# ====================================\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = x @ x\n",
    "    print(\"MPS test successful ✅\")\n",
    "else:\n",
    "    print(\"MPS not available ❌\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac499124",
   "metadata": {},
   "source": [
    "## Setup Workspace and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b92d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = Path.cwd().parent / \"data\"\n",
    "DIR_AUDIO = DIR_DATA / \"audio\"\n",
    "FILE_TEST_AUDIO = DIR_AUDIO / \"WhatsApp_Audio_2026-01-07_at_5_16_59_AM-2.wav\"\n",
    "\n",
    "# ====================================\n",
    "# LOAD ENV VARIABLES\n",
    "# ====================================\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b7ecd",
   "metadata": {},
   "source": [
    "## Preprocess Audio Files\n",
    "Convert to .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd2beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Remove spaces and periods from filename (except the extension period).\n",
    "    \n",
    "    Args:\n",
    "        filename: Original filename\n",
    "    \n",
    "    Returns:\n",
    "        Sanitized filename\n",
    "    \"\"\"\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    # Replace spaces and periods with underscores\n",
    "    name = name.replace(' ', '_').replace('.', '_')\n",
    "    return f\"{name}{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7962ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp4_to_wav(mp4_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Convert MP4 audio to WAV format.\n",
    "    \n",
    "    Args:\n",
    "        mp4_path: Path to the MP4 file (str or Path object)\n",
    "        output_dir: Directory to save the WAV file (defaults to same directory as MP4)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the converted WAV file\n",
    "    \"\"\"\n",
    "    mp4_path = Path(mp4_path)\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = mp4_path.parent\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sanitize the output filename\n",
    "    sanitized_name = sanitize_filename(f\"{mp4_path.stem}.wav\")\n",
    "    wav_path = output_dir / sanitized_name\n",
    "    \n",
    "    # Load audio from MP4 and save as WAV\n",
    "    audio, sr = librosa.load(mp4_path, sr=16000, mono=True)\n",
    "    sf.write(wav_path, audio, sr)\n",
    "    \n",
    "    print(f\"Converted {mp4_path.name} to {wav_path.name}\")\n",
    "    return wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# CONVERT EXAMPLE MP4 TO WAV\n",
    "# ====================================\n",
    "for file in DIR_AUDIO.iterdir():\n",
    "    if file.suffix == \".mp4\":\n",
    "        convert_mp4_to_wav(file, output_dir=DIR_AUDIO)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be4a2b",
   "metadata": {},
   "source": [
    "## Transcribe with Open Source Whisper Large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# LOAD WHISPER MODEL AND PROCESSOR\n",
    "# ====================================\n",
    "\n",
    "# Load model + processor (multilingual)\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "# Move to device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3741fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_whisper(audio_path, model, processor, device):\n",
    "    \"\"\"\n",
    "    Transcribe audio file using Whisper model with automatic language detection.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file\n",
    "        model: Whisper model\n",
    "        processor: Whisper processor\n",
    "        device: Device to run inference on ('mps' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Transcribed text\n",
    "    \"\"\"\n",
    "    # Load audio (Whisper expects 16kHz mono)\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Prepare input features\n",
    "    inputs = processor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_features = inputs.input_features.to(device)\n",
    "    \n",
    "    # Let Whisper auto-detect language\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d09788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Asiwe na mafuna uziwa kutia nga buweze banji ATM kadi. Yeo mwesi uguida nshiro."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================\n",
    "# TRANSCRIBE TEST AUDIO\n",
    "# ====================================\n",
    "# Test the function\n",
    "transcription = transcribe_with_whisper(FILE_TEST_AUDIO, model, processor, device)\n",
    "Markdown(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110d72b",
   "metadata": {},
   "source": [
    "## Transcribe with Commercial OpenAI API\n",
    "\n",
    "In addition to open-source Whisper, this notebook also evaluates transcription using OpenAI’s commercial speech-to-text models via the OpenAI API. These hosted models abstract away language detection and decoding details, allowing Chichewa audio to be transcribed directly without specifying a language token. The comparison therefore reflects practical, out-of-the-box performance of OpenAI’s continuously updated transcription service against open-source Whisper running locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18bb8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d1ea239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asibwe na mafuna uziwa kutianga buwezi banji ATM card Iyo mwesi uguira nshiro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sign in to OpenAI using Secrets in Colab\n",
    "\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "\n",
    "transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=FILE_TEST_AUDIO, response_format=\"text\")\n",
    "print(transcription)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
